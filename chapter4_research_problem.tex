\chapter{Fitted Q-Iteration with Deep State-Dynamics Features}
\label{ch3_setup}
\thispagestyle{empty}

\vspace{0.5cm}

As central argument of this thesis, we propose a DRL method which 
combines the feature extraction capabilities of deep CNNs with the quick 
and powerful batch RL approach of FQI. 
Given a high-dimensional state space of pixels representing sequences of 
greyscale frames from an Atari game, we use a deep convolutional autoencoder 
to map the original state space to a compressed \textit{feature space} which 
accounts for both the states and their one-step dynamics (i.e. the transition 
model). This compressed representation is then used to run a \textit{tree-based}
FQI algorithm in batch mode.

In this chapter we give a formal description of the method and its core 
components. Technical details of implementation will be discussed in the next
chapter.

\section{Motivation}
% TODO: remove part on dynamics1
The state-of-the-art DRL methods listed in the previous chapter are able to 
outperform classic RL algorithms in a wide variety of problems, and in some 
cases are the only possible way of dealing with high-dimensional control 
settings like the Atari games. 
However, the representations learned by deep approximators in DRL are often 
tailored to the problem and give little more information than that required 
to learn a very specific function of the state-action space. Anderson et al.\ 
\cite{anderson2015faster} try to partially address this problem with good 
results in terms of convergence, indicating that RL methods work better when 
provided with true information about the environment rather than by sheer
brute-force.
Moreover, the approaches cited above tend to be grossly 
\textit{sample-inefficient}, requiring tens of millions of samples collected
on-line to reach optimal performance. Several publications successfully deal 
with this aspect, but nonetheless leave room for improvement (lowering at most
by one order of magnitude the number of samples required).
The method introduced by Lange and Riedmiller (2010) \cite{lange2010deep} is 
similar to ours but lacks in extracting useful information about the dynamics
of the environment; moreover, their dense architecture predates the more modern
convolutional approaches in image processing and is less suited for complex
tasks than our AE.

The method that we propose tries to improve both aspects of information content
of the compressed feature space and sample efficiency. We extract general 
features from the environments and try to reach better or equivalent performance
in up to two orders of magnitude less samples than DQN on Atari games.

\section{Problem Formulation}
% TODO: add RFS
The general setting of this problem is typical of DRL problems: we use a deep 
ANN to extract a representation of an environment, and use that representation 
to control an agent with standard RL algorithms. 

In our approach we use a modular architecture with two separate stages for the
training phase, and combine the two stages in an end-to-end fashion during
the control phase. The two main components of the algorithm are:
%
\begin{enumerate}
    \item a deep convolutional autoencoder which we use to extract a 
    representation of the environment;
    the purpose of the AE is to map the original, pixel-level state space $S$ of
    the environment into a heavily compressed feature space ${\tilde{S}}$ which
    contains information of both the state space and the transition model of the
    environment;
    \item a \textit{tree-based} FQI learning algorithm which produces an 
    estimator for the action-value function, with $\tilde{S}$ as domain. 
    The estimator is then used for standard greedy control.
\end{enumerate}
%
% Combine the two components
The two main components are separately trained to produce the two 
transformations $ENC: S \rightarrow \tilde{S}$ and $\tilde{Q}^\pi: \tilde{S} 
\times A \rightarrow \mathbb{R}$. After the training phase, we simply combine 
the two functions to obtain the action-value function $Q^\pi: S \times A 
\rightarrow \mathbb{R}$ as follows: 
%
\begin{IEEEeqnarray}{rCl}
    %
    Q^\pi(s, a) = \tilde{Q}^\pi(ENC(s), a)
    %
\end{IEEEeqnarray}
%
A general description of the process is given in Algorithm \ref{alg:FQI-DSDF}.
%
\begin{algorithm}[h]
    \caption{Fitted Q-Iterations with Deep State-Dynamics Features}
    \label{alg:FQI-DSDF}
    \begin{algorithmic}
	\STATE \textbf{Given}: an arbitrary policy $\pi$;
	\STATE Initialize the encoder $ENC: S \rightarrow \tilde{S}$ arbitrarily;
	\STATE Initialize the decoder $DEC: \tilde{S} \rightarrow S$ arbitrarily;
	\REPEAT 
	    \STATE Collect a set $\mathcal{TS}$ of four-tuples $(s \in S, a \in A, r \in \mathbb{R}, s' \in S)$ using $\pi$;
	    \STATE Train the composition $DEC \circ ENC: S \rightarrow S$ using the first column of $\mathcal{TS}$ as input and the last column as target;
	    \STATE Build a set $\mathcal{TS}_F$ of four-tuples $(f \in \tilde{S}, a \in A, r \in \mathbb{R}, f' \in \tilde{S})$ by applying the encoder to the first and last column of $\mathcal{TS}$ s.t. $f = ENC(s)$;
	    \STATE Call FQI on $\mathcal{TS}_F$ to produce $\tilde{Q}^\pi: \tilde{S} \times A \rightarrow \mathbb{R}$;
	    \STATE Combine $\tilde{Q}^\pi$ and $ENC$ to produce $Q^\pi: S \times A \rightarrow \mathbb{R}$:
		\[
		Q^\pi(s, a) = \tilde{Q}^\pi(ENC(s), a)
		\]
	    \STATE Set $\pi(s) = \underset{a}{\arg\max} Q^{\pi}(s, a)$;
	\UNTIL{stopping condition is met;}
    \end{algorithmic}
\end{algorithm}
%
\section{Extraction of State Features}
% TODO: remove part on dynamics
The AE used in our approach consists of two main components, namely an 
\textit{encoder} and a \textit{decoder} (cf. \ref{s:AE}). To the end of 
explicitly representing the encoding purpose of the AE, we keep a separate 
notation of the two modules; we therefore refer to two different CNNs, namely 
$ENC: S \rightarrow \tilde{S}$ that maps the original state space to the 
compressed representation $\tilde{S}$, and $DEC: \tilde{S} \rightarrow S$ which
performs the inverse transformation. The full AE is the composition of the two 
networks $AE: DEC \circ ENC: S \rightarrow S$. Note that the composition is 
differentiable end-to-end, and basically consists in \textit{plugging} the last
layer of the encoder as input to the decoder. 

We train the AE to extract\footnote{By \textit{extraction} we mean the 
transformation computed by the encoder.} a joint representation of the 
environment's state and dynamics by SGD, using a training set of $(s, s')$ 
tuples of consecutive states collected with a sufficiently explorative policy. 
The AE must therefore learn to map a state to its successor under the sampling 
policy. 

This ...

\section{Tree-based Recursive Feature Selection}
% TODO

\section{Tree-based Fitted Q-Iteration}
% TODO