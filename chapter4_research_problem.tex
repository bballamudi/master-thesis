\chapter{Fitted Q-Iteration with Deep State Features}
\label{ch3_setup}
\thispagestyle{empty}

\vspace{0.5cm}

As central argument of this thesis, we propose a DRL method which 
combines the feature extraction capabilities of deep CNNs with the quick 
and powerful batch RL approach of FQI. 
Given a high-dimensional state space of pixels representing sequences of 
greyscale frames from an Atari game, we use a deep convolutional autoencoder 
to map the original state space to a compressed \textit{feature space} which 
accounts for the states and its nominal state dynamics (i.e.\ those changes not
directly influenced by the agent). We reduce the representation further by 
applying a feature selection algorithm to the extracted state features and this 
final compressed representation is then used to run a \textit{tree-based} 
version of FQI. We repeat this procedure iteratively resulting in a 
\textit{semi-batch} algorithm. 

In this chapter we give a formal description of the method and its core 
components. Technical details of implementation will be discussed in the next
chapter.

\section{Motivation}
The state-of-the-art DRL methods listed in the previous chapter are able to 
outperform classic RL algorithms in a wide variety of problems, and in some 
cases are the only possible way of dealing with high-dimensional control 
settings like the Atari games. 
However, the approaches cited above tend to be grossly 
\textit{sample-inefficient}, requiring tens of millions of samples collected
on-line to reach optimal performance. Several publications successfully deal 
with this aspect, but nonetheless leave room for improvement (lowering at most
by one order of magnitude the number of samples required).
The method introduced by Lange and Riedmiller (2010) \cite{lange2010deep} is 
similar to ours but their dense architecture predates the more modern 
convolutional approaches in image processing and is less suited for complex
tasks than our AE.

The method that we propose tries to improve both aspects of information content
of the compressed feature space and sample efficiency. We extract general 
features from the environments and try to reach better or equivalent performance
in up to two orders of magnitude less samples than DQN on Atari games.

\section{Algorithm Description}
The general structure of this algorithm is typical of DRL settings: we use a 
deep ANN to extract a representation of an environment, and use that 
representation to control an agent with standard RL algorithms. We also add an 
additional step after the deep feature extraction to further reduce the 
representation down to the essential bits of information required to solve the 
problem by using the \textit{Recursive Feature Selection} (RFS) algorithm 
\cite{castelletti2011tree}.

We focus exclusively on environments with a discrete and mono-dimensional 
action space $A$, where actions are assigned a unique integer identifier 
starting from $0$ with no particular order. 
We also assume to be operating in a three dimensional state space 
($C \times W \times H$\footnote{Channels times Width times Height}) for 
consistency with the experimental setting on which we tested the algorithm (with
pixel-level state spaces), although in general the algorithm requires no such 
assumption and could be easily adapted to higher or lower dimensional settings. 

The algorithm uses a modular architecture with three different components which 
are combined after training to produce an approximator of the action-value 
function. The main components of the algorithm are:
%
\begin{enumerate}
    \item a deep convolutional autoencoder which we use to extract a 
    representation of the environment;
    the purpose of the AE is to map the original, pixel-level state space $S$ of
    the environment into a strongly compressed feature space ${\tilde{S}}$ which
    contains information of both the state space and part of the transition 
    model of the environment;
    \item the \textit{Recursive Feature Selection} (RFS) technique to further 
    reduce the state representation $\tilde{S}$ and keep only the truly 
    informative features extracted by the AE, effectively mapping the extracted 
    state-space to a subspace $\hat{S}$.
    \item the \textit{tree-based} FQI learning algorithm which produces an 
    estimator for the action-value function, with $\hat{S}$ as domain. 
\end{enumerate}
%

The full procedure consists in alternating a training step and an evaluation 
step, until the desired performance is reached.

A training step of the algorithm takes as input a training set $\mathcal{TS}$ of
four-tuples $(s \in S, a \in A, r \in \mathbb{R}, s' \in S)$ and produces a new 
approximation of the action-value function, and consists in sequentially 
training the three components from scratch to produce the following 
transformations respectively:
\begin{itemize}
    \item $ENC: S \rightarrow \tilde{S}$, from the pixel representation to a 
    compressed feature space;
    \item $RFS: \tilde{S} \rightarrow \hat{S}$, from the compressed feature space
    to a minimal subspace with the most informative features;
    \item $\hat{Q}: \hat{S} \times A \rightarrow \mathbb{R}$, an approximation
    of the optimal action-value function on $\hat{S}$.
\end{itemize}
After training, we simply combine the three functions to obtain the full
action-value function $Q: S \times A \rightarrow \mathbb{R}$ as follows: 
%
\begin{IEEEeqnarray}{rCl}
    %
    Q(s, a) = \hat{Q}(RFS(ENC(s)), a) \label{eq:final_output}
    %
\end{IEEEeqnarray}
%

To collect the training set, we define a greedy policy $\pi$ based on the 
current approximation of $Q$ as:
%
\begin{IEEEeqnarray}{rCl}
    %
    \pi(s) = \underset{a}{\arg\max} Q(s, a)
    %
\end{IEEEeqnarray}
%
and we use an $\varepsilon$-greedy policy $\pi_\varepsilon$ based on $\pi$ 
(cf.\ Section \ref{s:policies}) to collect $\mathcal{TS}$.
We initialize the $\varepsilon$-greedy policy as fully random (which means that
we do not need an approximation of $Q$ for the first step), and we decrease 
$\varepsilon$ after each step down to a fixed minimum positive value 
$\varepsilon_{min}$. 
This results in a sufficiently high exploration at the beginning of the 
procedure, but increasingly exploits the learned knowledge to improve the 
quality of the collected samples after each training step, as the agent learns 
to reach states with a higher value. The lower positive bound on $\varepsilon$ 
is kept to minimize overfitting and allow the agent to explore potentially 
better states even in the later steps of the algorithm.
A similar approach, called \textit{$\varepsilon$-annealing}, was used by Mnih et
al.\ (2015) \cite{mnih2015human} for the online updates in DQN.

Each training step is followed by an evaluation phase to determine the quality 
of the learned policy and eventually stop the procedure when the performance is 
deemed satisfactory.

A general description of the process is reported in Algorithm \ref{alg:FQI-DSDF},
and details on the training phases and evaluation step are given in the 
following sections. 
%
\begin{algorithm}[h]
    \caption{Fitted Q-Iteration with Deep State Features}
    \label{alg:FQI-DSDF}
    \begin{algorithmic}
	\STATE Given: 
	    \begin{ALC@g}
	        \STATE $\varepsilon_{min} \in (0, 1)$
	        \STATE $\varphi: [\varepsilon_{min}, 1] \rightarrow [\varepsilon_{min}, 1]$ s.t.\ $\varphi(x) < x, \forall x \in (\varepsilon_{min}, 1]$ and $\varphi(\varepsilon_{min}) = \varepsilon_{min}$;
	    \end{ALC@g}
	\STATE Initialize the encoder $ENC: S \rightarrow \tilde{S}$ arbitrarily;
	\STATE Initialize the decoder $DEC: \tilde{S} \rightarrow S$ arbitrarily;
	\STATE Initialize $Q$ arbitrarily;
	\STATE Define $\pi(s) = \underset{a}{\arg\max} Q(s, a)$;
	\STATE Initialize an $\varepsilon$-greedy policy $\pi_\varepsilon$ based on $\pi$ with $\varepsilon = 1$;
	\REPEAT 
	    \STATE Collect a set $\mathcal{TS}$ of four-tuples $(s \in S, a \in A, r \in \mathbb{R}, s' \in S)$ using $\pi_\varepsilon$;
	    \STATE Train the composition $DEC \circ ENC: S \rightarrow S$ using the first column of $\mathcal{TS}$ as input and target;
	    \STATE Build a set $\mathcal{TS}_{ENC}$ of four-tuples $(\tilde{s} \in \tilde{S}, a \in A, r \in \mathbb{R}, \tilde{s}' \in \tilde{S})$ by applying the encoder to the first and last column of $\mathcal{TS}$ s.t. $\tilde{s} = ENC(s)$;
	    \STATE Call the RFS feature selection algorithm on $\mathcal{TS}_{ENC}$ to obtain a space reduction $RFS: \tilde{S} \rightarrow \hat{S}$;
	    \STATE Build a set $\mathcal{TS}_{RFS}$ of four-tuples $(\hat{s} \in \hat{S}, a \in A, r \in \mathbb{R}, \hat{s}' \in \hat{S})$ by applying $RFS$ to the first and last column of $\mathcal{TS}_{ENC}$ s.t. $\hat{s} = RFS(\tilde{s})$;
	    \STATE Call FQI on $\mathcal{TS}_{RFS}$ to produce $\hat{Q}: \hat{S} \times A \rightarrow \mathbb{R}$;
	    \STATE Combine $\hat{Q}$, $RFS$ and $ENC$ to produce $Q: S \times A \rightarrow \mathbb{R}$:
		\[
		    Q(s, a) = \hat{Q}(RFS(ENC(s)), a)
		\]
	    \STATE Set $\varepsilon = \varphi(\varepsilon)$;
	    \STATE Evaluate $\pi$; 
	\UNTIL{stopping condition on evaluation performance is met;}
	\STATE Return $Q$
    \end{algorithmic}
\end{algorithm}
%
Note that in general the \textit{semi-batch} approach which starts from a 
training set collected under a fully random policy and sequentially collects new
dataset as performance improves is not strictly necessary. It is also possible
to run a single training step in pure batch mode, using a dataset collected 
under an expert policy (albeit with some degree of exploration required by FQI) 
to produce an approximation of the optimal $Q$ function in one step. 
This allows to exploit the sample efficiency of the algorithm in situations 
where, for instance, collecting expert samples is expensive or difficult.

\section{Extraction of State Features}
The AE used in our approach consists of two main components, namely an 
\textit{encoder} and a \textit{decoder} (cf.\ Section \ref{s:AE}). To the end of 
explicitly representing the encoding purpose of the AE, we keep a separate 
notation of the two modules; we therefore refer to two different CNNs, namely 
$ENC: S \rightarrow \tilde{S}$ that maps the original state space to the 
compressed representation in $\tilde{S}$, and $DEC: \tilde{S} \rightarrow S$ 
which performs the inverse transformation. The full AE is the composition of the
two networks $AE: DEC \circ ENC: S \rightarrow S$. Note that the composition is 
differentiable end-to-end, and basically consists in \textit{plugging} the last
layer of the encoder as input to the decoder. 

We train the AE ...

\section{Tree-based Recursive Feature Selection}
% TODO

\section{Fitted Q-Iteration}
The last component of our training pipeline is the Fitted Q-Iteration algorithm 
(cf.\ \ref{s:FQI}).
We provide as input to the procedure a new training set $\mathcal{TS}_{RFS}$
of four-tuples $(\hat{s} \in \hat{S}, a \in A, r \in \mathbb{R}, \hat{s}' \in \hat{S})$, 
obtained by applying the $RFS$ transformation to the first and last column of 
$\mathcal{TS}_{ENC}$. 
We train the model to output a multi-dimensional estimate of the action-value
function on $\mathbb{R}^{|A|}$, with one value for each action that the agent 
can take in the environment, and we restrict the output to a single value on 
$\mathbb{R}$ using the action identifier as index (e.g.\ $Q(s, 0)$ will return 
the first dimension of the model's output, corresponding to the value of action
0 in state $s$). 
The output of this training phase is the transformation 
$\hat{Q}: \hat{S} \times A \rightarrow \mathbb{R}$, which is then combined with 
$ENC$ and $RFS$ as per Equation \eqref{eq:final_output} to produce the next
approximation of $Q$. This phase also concludes the training step.

% TODO: image of the full model AE + RFS + FQI

