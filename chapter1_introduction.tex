\chapter{Introduction}
\label{ch1_intro}
\thispagestyle{empty}

\vspace{0.5cm}
 
Human behavior is a complex mix of elements. We are subject to 
well defined biological and social forces, but at the same time our actions are 
deliberate, our thinking is abstract, and our objectives can be irrational; 
our brains are extremely complex interconnections, but of extremely simple 
building blocks; we are crafted by ages of random evolution, but at the same 
time of precise optimization. 
When we set out to describe the inner workings of human cognition, we must 
consider all of these colliding aspects and account for them in our work, 
without overcomplicating what is simple or approximating what is complex.

Artificial intelligence is a broad field, that looks at the variegate spectrum 
of human behavior to replicate our complexity in a formal or algorithmic way. 
Among the techniques of AI, one family of algorithms called \textit{machine
learning} is designed to give computers the ability to \textit{learn} to carry 
out a task without being explicitly programmed to do so. 
The focus on different types of tasks is what defines the many subfields of 
machine learning.
For instance, the use of machine learning algorithms to take decisions in a 
\textit{sequential decision-making problem} is called \textit{reinforcement 
learning}, whereas \textit{deep learning} is a family of techniques to learn an 
abstract representation of a vector space (e.g.\, learning how to describe an
image from the value of its pixels).

Teaching machines to behave like human beings is a complex and inspiring problem
on its own; however, an even more complex task is not to simply teach 
computers to mimic humans, but to do so with the same learning 
\textit{efficiency} of the biological brain, which is able to solve problems by
experiencing them very few times (sometimes even imagining a problem is enough 
to solve it, by comparing it with other similar known situations). 
The purpose of this thesis is to make a step in this direction by
combining powerful machine learning techniques with efficient ones, in order
to tackle complex control problems without needing many examples to learn
from.

\section{Goals and Motivation}
The source of inspiration for this work comes from the recent progress in 
reinforcement learning associated to \textit{deep reinforcement learning} (DRL)
techniques. 
Among these, we cite \textit{Deep Q-Learning} by Mnih et al.\ \cite{mnih2015human},
the \textit{Asynchronous Advantage Actor-Critic} algorithm also by Mnih et 
al.\ \cite{mnih2016asynchronous}, and the \textit{Neural Episodic Control} 
algorithm by Pritzel et al.\ \cite{pritzel2017neural}.
All these algorithms (which were all the state of the art at the time of
their respective publication), are based on deep learning to extract 
the visual characteristics of an environment, and then using those 
characteristics to learn a policy with well-known reinforcement learning 
techniques. A major drawback of these approaches, however, is that they all 
require an enormous amount of experience to converge, which may prove unfeasible
in real-world problems where the collection of examples is expensive.
The main purpose of this work is to explore the limits of this apparent 
requirement, and to propose an algorithm that is able to perform well even under 
such scarcity conditions.

\section{Proposed Solution}
We introduce a deep reinforcement learning agent that extracts an abstract 
description of the environment starting from its visual characteristics, and 
then leverages this description to learn a complex behavior using a small 
collection of training examples.

We devise a combination of different machine learning techniques. We use 
\textit{unsupervised} deep learning to extract a representation of the visual 
characteristics of the environment, as well as a \textit{feature selection}
procedure to reduce this representation while keeping all the necessary 
information for control, and a \textit{batch} reinforcement learning algorithm 
which exploits this filtered representation to learn a \textit{policy}.
The result is a reinforcement learning procedure that requires a small amount
of experience in order to learn, and which produces an essential and 
general representation of the environment as well as a well-performing control 
policy for it.

We test our algorithm on the \textit{Atari games} environments, which are
the \textit{de facto} standard test bench of deep reinforcement learning, and we 
also perform an experimental validation of all the components in our learning 
pipeline in order to ensure the quality of our approach.
We test the suitability of the extracted representation to both describe the 
environment and provide all the information necessary for control, as well as 
we gain useful knowledge on some environments by looking at the feature 
selection.

We compare our results with the DQN algorithm by Mnih et al.\ \cite{mnih2015human},
and we find that our agent is able to learn non-trivial policies on the tested 
environments, but that it fails to converge to the same performance of DQN. 
However, we show that the number of samples required by our procedure to reach 
its maximum performance is up to two orders of magnitude smaller than that 
required by DQN, and that our agent's performance is on average eight times 
higher when limiting the training of the two methods to this small number of 
samples.

\section{Original Contributions}
The use of unsupervised deep learning to extract a feature space for control 
purposes had already been proposed by Lange and Riedmiller \cite{lange2010deep} 
before the field of deep reinforcement learning really took off in the early 
2010s. 
However, their approach used a different architecture for the feature extraction, 
which would be inefficient for complex control problems like the ones we test
in this work. 

Moreover, to the best of our knowledge there is no work in the literature that
applies a feature selection technique to deep features explicitly targeted 
for control purposes. 
This additional step allows us to optimize our feature extraction process for 
obtaining a generic description of the environment rather than a task-specific 
one, while at the same time improving the computational requirements of our 
agent by keeping only the essential information. 

Our algorithm makes a step towards a more sample-efficient deep reinforcement 
learning, and would be a better pick than DQN in situations characterized by a 
lack of training samples and a big state space. 

\section{Thesis Structure}
The thesis is structured as follows. 

In Chapter \ref{chapter2_background} we provide the theoretical framework on
which our algorithm is based, and we give an overview of the most important 
concepts and techniques in the fields of deep learning and reinforcement 
learning.

In Chapter \ref{chapter3_state_of_the_art} we summarize the state-of-the-art 
results in the field of deep reinforcement learning.

In Chapter \ref{chapter4_research_problem} we introduce our deep reinforcement 
learning algorithm, and provide a formal description of its components and 
their behavior.

In Chapter \ref{chapter5_technical_details} we describe in detail the specific 
implementation of each module in the algorithm, with architectural choices, 
model hyperparameters, and training configurations.

In Chapter \ref{chapter6_experiments} we show experimental results from running
the algorithm on different environments.

In Chapter \ref{chapter7_conclusions} we summarize our work and discuss possible
future developments and improvements.


