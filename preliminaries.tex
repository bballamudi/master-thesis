\chapter{Background}
\label{ch1_intro}
\thispagestyle{empty}

\vspace{0.5cm}

\noindent Introduce the section

\section{Deep Learning} \label{s:DL}
Deep Learning (DL) is a branch of machine learning which exploits \textit{Artificial 
Neural Networks} (ANN) with more than one hidden layer to learn an abstract 
representation of the input space \cite{lecun2015deep}. 

Deep learning techniques can be applied to the three main classes of problems 
of machine learning (supervised, semi-supervised, and unsupervised), and have 
been used to achieve state-of-the-art results in a variety of learning tasks.

\subsection{Artificial Neural Networks}
Feed-forward Artificial Neural Networks (ANN) are function approximators 
inspired by the connected structure of neurons and synapses in the animal brain.

\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNN) are a type of ANN inspired by the visual 
cortex in animal brains.
CNNs exploit spatially-local correlations in the neurons of adjacent 
layers through the use of a \textit{receptive field}, a set of weights which is 
used to transform a local subset of the input neurons of a layer.

% shared weights
% put some images here

\subsection{Autoencoders}
Autoencoders are a type of ANN which are used to learn a sparse and compressed 
representation of the input space, by sequentially compressing and 
reconstructing the inputs under some sparsity constraint (typically by 
minimizing the $L1$ norm of the activations in the innermost hidden layers).


\section{Reinforcement Learning} \label{s:DRL}

Introduction to reinforcement learning...

\subsection{Markov Decision Processes}
Markov Decision Processes (MDP) are discrete-time, stochastic control 
processes, that can be used to describe the interaction of an \textit{agent} 
with an \textit{environment}.

Formally, MDPs are defined as 7-tuples $(S, S^{t}, A, P, R, \gamma, \mu)$, 
where:
\begin{itemize}
    %
    \item $S$ is the set of observable states of the environment. \\
    When the set observable states coincides with the true set of states of the 
    environment, the MDP is said to be \textit{fully observable}. We will only deal 
    with fully observable MDPs without considering the case of \textit{partially 
    observable} MDPs.

    \item $S^{T} \subseteq S$ is the set of \textit{terminal states} of the 
    environment, meaning those states in which the interaction between the agent and 
    the environment ends. A sequence of states observed by an agent during an 
    interaction with the environment and ending in a terminal state is usually 
    called an \textit{episode}.
 
    \item $A$ is the set of actions that the agent can execute in the environment.
 
    \item $P: S \times A \times S \rightarrow [0,1]$ is a \textit{state transition 
    function} which, given two states $s, s' \in S$ and an action $a \in A$, 
    represents the probability of the agent going to state $s'$ by executing $a$ in 
    $s$.
 
    \item $R: S \times A \rightarrow \mathbb{R}$ is a \textit{reward function} 
    which represents the reward that the agent collects by executing an action in 
    a state. 
    
    \item $\gamma \in (0,1)$ is a \textit{discount factor} with which the rewards 
    collected by the agent are diminished at each step, and can be interpreted as 
    the agent's interest for rewards further in time rather than immediately.
    
    \item $\mu: S \rightarrow [0, 1]$ is a probability distribution over $S$ which 
    models the probability of starting the exploration of the environment in a 
    given state.
    %
\end{itemize}

Episodes are usually represented as sequences of tuples 
\[
    [(s_0, a_0, r_1, s_1), ..., (s_{n-1}, a_{n-1}, r_n, s_n)]
\]
called \textit{trajectories}, where $(s_i, a_i, r_{i+1}, s_{i+1})$ represent a
transition of the agent to state $s_{i+1}$ by taking action $a_i$ in $s_i$ and 
collecting a reward $r_{i+1}$, and $s_n \in S^T$.

In Markov Decision Processes the modeled environment must satisfy the 
\textit{Markov property}, meaning that the reward and transition functions of 
the environment must only depend on the current state and action, rather than 
the past state-action trajectory of the agent.

In other words, an environment is said to satisfy the Markov property when its 
one-step dynamics allow to predict the next state and reward given only the 
current state and action.

\subsubsection{Policy}
The behavior of the agent in an MDP can be defined as a probability 
distribution $\pi: S \times A \rightarrow [0,1]$ called a \textit{policy}, 
which given $s \in S, a \in A$, represents the probability of selecting $a$ as 
next action from $s$.

An agent which uses this probability distribution to select its next action 
when in a given state is said to be \textit{following} the policy.

\subsubsection{Value Functions}
Starting from the concept of policy, we can now introduce a function that 
evaluates how good it is for an agent following a policy $\pi$ to be in a given 
state. This evaluation is expressed in terms of the expected return, i.e.
the expected discounted sum of future rewards collected by an agent starting 
from a state while following $\pi$, and the function that computes it is 
called the \textit{state-value function for policy $\pi$} (or, more commonly, 
simply \textit{value function}).

Formally, the state-value function associated to a policy $\pi$ is a function 
$V^{\pi}: S \rightarrow \mathbb{R}$ defined as:
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    V^{\pi}(s) & = & E_\pi[R_t | s_t = s] \\
    & = & E_\pi[\sum\limits_{k = 0}^{\infty} \gamma^k r_{t+k+1} | s_t = s]
    %
\end{IEEEeqnarray}
%
where $E_\pi[\cdot]$ is the expected value given that the agent follows 
policy $\pi$, and $t$ is any time step of an episode $[s_1, ..., s_t, ..., s_n]$
where $s_t \in S, \forall t = 1, ..., n$.

Similarly, we can also introduce a function that evaluates the goodness of 
taking a specific action in a given state, namely the expected reward obtained 
by taking an action $a \in A$ in a state $s \in S$ and then following policy $\pi$. 
We call this function the \textit{action-value function for policy $\pi$} 
denoted $Q^{\pi}: S \times A \rightarrow \mathbb{R}$, and defined as: 
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    Q^{\pi}(s, a) & = & E_\pi[R_t | s_t = s, a_t = a] \\
    & = & E_\pi[\sum\limits_{k = 0}^{\infty} \gamma^k r_{t+k+1} | s_t = s, a_t = a]
    %
\end{IEEEeqnarray}
%
The majority of reinforcement learning algorithms is based on computing (or 
estimating) value functions to then control the behavior of the agent in order 
to maximize the expected reward collected during episodes.

We also note a fundamental property of value functions, which satisfy particular 
recursive relationships like the following \textit{Bellman equation for $V^{\pi}$}:
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    V^{\pi}(s) & = & E_\pi[R_t | s_t = s] \nonumber\\
    & = & E_\pi[\sum\limits_{k = 0}^{\infty} \gamma^k r_{t+k+1} | s_t = s] \nonumber\\
    & = & E_\pi[r_{t+1} + \gamma \sum\limits_{k=0}^{\infty} \gamma^k r_{t+k+2} | s_t = s] \\
    & = & \sum\limits_{a \in A} \pi(s, a) \sum\limits_{s' \in S} P(s, a, s')[R(s, a) \>+ \nonumber\\
    && +\> \gamma E_\pi[\sum\limits_{k=0}^{\infty} \gamma^k r_{t+k+2} | s_{t+1} = s']] \\
    & = & \sum\limits_{a \in A} \pi(s, a) \sum\limits_{s' \in S} P(s, a, s')[R(s, a) + \gamma V^{\pi}(s')] \label{eq:BEV}
    %
\end{IEEEeqnarray}
%
Intuitively, relation \eqref{eq:BEV} decomposes the state-value function as the sum of the
immediate reward collected from a state $s$ to a successor state $s'$, and the 
value of $s'$ itself; by considering the transition model of the MDP and the 
policy being followed, we see that the Bellman equation simply averages the 
expected return over all the possible $(s, a, r, s')$ transitions, by taking 
into account the probability that these transitions occur. 

\subsection{Optimal Value Functions}
In general terms, \textit{to solve} a reinforcement learning task is to identify
a policy that yields a sufficiently high expected return. In the case of MDPs 
with finite state and actions sets\footnote{This specification is only required 
for formality, but is not expanded further in this work. Refer to SUTTON, BARTO 
for more details on the subjet of non-finite MDPs.}, it is possible to define 
the concept of \textit{optimal policy} as the policy which maximizes the 
expected return collected by the agent in an episode.

We start by noticing that state-value function defines a partial ordering over 
policies as follows: 
\[
    \pi \ge \pi' \iff V^{\pi}(s) \ge V^{\pi'}(s), \forall s \in S
\]
From this, the \textit{optimal policy $\pi^*$} of an MDP is a policy which is
better or equal than all other policies in the policy space.

The state-value function associated to $\pi^*$ is called the 
\textit{optimal state-value function}, denoted $V^*$ and defined as:
\[
    % Sutton, Barto
    V^*(s) = \max_{\pi} V^\pi(s), \forall s \in S
\]
As we did when introducing the value functions, given an optimal policy for the 
MDP it is also possible to define the \textit{optimal action-value function} 
denoted $Q^*$ (equivalence \eqref{eq:Qstar_V} in this definition highlights the
relation between $Q^*$ and $V^*$):
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    Q^*(s, a) & = & \max_{\pi} Q^\pi(s, a) \\
    & = & E[r_{t+1} + \gamma V^*(s_{t+1} | s_t = s, a_t = a] \label{eq:Qstar_V}
    %
\end{IEEEeqnarray}
%
.

%% SEE PAGE 224 of Sutton, Barto and ask Restelli WTF is going on with the definition of V*
Since $V^*$ and $Q^*$ are value functions of an MDP, they must satisfy the same
type of recursive relations that we described in \eqref{eq:BEV}, in this case
called the \textit{Bellman optimality equations}.

The Bellman optimality equation for $V^*$ expresses the fact that the value of
a state associated to an optimal policy must be the expected return of the best
action that the agent can take in that state:
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    V^*(s) & = & \max_a Q^*(s, a) \label{eq:BOEV}\\
    & = & \max_a E_{\pi^*}[R_t | s_t = s, a_t = a] \\
    & = & \max_a E_{\pi^*}[\sum\limits_{k=0}^{\infty} \gamma^k r_{t+k+1}| s_t = s, a_t = a] \\
    & = & \max_a E_{\pi^*}[r_{t+1} + \gamma \sum\limits_{k=0}^{\infty} \gamma^k r_t+k+2 | s_t = s, a_t = a] \\
    & = & \max_a E_{\pi^*}[r_{t+1} + \gamma V^*(s_{t+1}) | s_t = s, a_t = a] \\
    & = & \max_a \sum\limits_{s' \in S} P(s, a, s') [ R(s, a) + \gamma V^*(s') ]
    %
\end{IEEEeqnarray}
%
The Bellman optimality equation for $Q^*$ is again obtained from the definition
as:
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto
    Q^*(s, a) & = & E[ r_{t+1} + \gamma \max_{a'} Q^*(s_{t+1}, a') | |s_t = s, a_t = a] \\
    & = & \sum\limits_{s'} P(s, a, s') [ R(s, a) + \gamma \max_{a'}Q^*(s', a') ]
    %
\end{IEEEeqnarray}
%
It is possible to notice that both Bellman optimality equations have a unique
solution independent of the policy. 
If the dynamics of the environment ($R$ and $P$) are fully known, it is possible
to solve the system of equations associated to the value functions (i.e. one
equation for each state in $S$) and get an exact value for $V^*$ and $Q^*$ in 
each state. 

\subsection{Value-based optimization}
One of main algorithm classes for solving reinforcement learning problems
is based on searching an optimal policy for the MDP by trying to compute either 
of the optimal value functions, and then deriving a policy based on them.

From $V^*$ or $Q^*$, it is easy to determine an optimal policy:
\begin{itemize}
    %
    \item Given $V^*$, for each state $s \in S$ there will be an action (or 
    actions) which maximizes the Bellman optimality equation \eqref{eq:BOEV}. 
    Any policy that assigns positive probability to only this action is an 
    optimal policy. \\
    This approach therefore consists in performing a one-step forward search on 
    the state space to determine the best action from the current state.
    \item Given $Q^*$, the optimal policy is that which assigns positive 
    probability to the action which maximizes $Q^*(s, a)$; this approach 
    exploits the intrinsic property of the action-value function of representing 
    the \textit{goodness} of actions, without performing the one-step search 
    on the successor states. 
    %
\end{itemize}

In this section we will describe some of the most important value-based 
approaches to reinforcement learning, which will be useful in the following 
sections of this work. 

We will not deal with equally popular methods like \textit{policy gradient} or 
\textit{actor-critic} approaches, even though they have been successfully applied
in conjunction with deep learning to solve complex environments (see \ref{s:DRL}
and \ref{ch2_SOA}).

\subsection{Dynamic Programming}
The use of dynamic programming (DP) techniques to solve a reinforcement learning 
problem is based on recursively applying some form of the Bellman equation, 
starting from any initial policy $\pi$ until convergence to $\pi^*$.

In this class of algorithms, we identify two main approaches: \textit{policy 
iteration} and \textit{value iteration}.

\subsubsection{Policy iteration}
\textit{Policy iteration} is based of the following theorem:
\begin{theorem}[Policy improvement theorem] \label{th:pol_imp}
    % Lecture 11, slide 18
    Let $\pi$ and $\pi'$ be a pair of deterministic policies such that
    \[
        Q^\pi(s, \pi'(s)) \ge V^\pi(s), \forall s \in S 
    \]
    Then, $\pi' \ge \pi$, i.e. 
    \[
        V^{\pi'}(s) \ge V^{\pi}(s), \forall s \in S
    \]
\end{theorem}

This approach works by iteratively computing the value functions associated to 
the current policy, and then improving that policy by making it act greedily 
with respect to the value functions, such that:
%
\begin{IEEEeqnarray}{rCl}
    % Lecture 11, slide 17
    \pi'(s) = \underset{a \in A}{\arg\max} Q^{\pi}(s, a) \label{eq:greedy_imp}
    %
\end{IEEEeqnarray}
%
which, for Theorem \ref{th:pol_imp}, improves the expected return of the policy
because:
\[	
    % Lecture 11, slide 17
    Q^\pi(s, \pi'(s)) = \max_{a \in A} Q^\pi(s, a) \ge Q^\pi(s, \pi(s)) = V^\pi(s)
\]
. 

This continuous improvement is applied until the inequality in the previous 
equation becomes an equality, i.e. when the improved policy satisfies the 
Bellman optimality equation \eqref{eq:BOEV}. Since the algorithm gives no 
assurances on the number of updates required for convergence, some stopping
conditions are usually introduced to end the process when the new value function 
did not change substantially after the update (\textit{$\epsilon$-convergence}) 
or a certain threshold number of iterations has been reached.

\subsubsection{Value iteration} \label{s:value_iteration}
Starting from a similar basis, the \textit{value iteration} approach computes 
the value function associated to an initial policy, but then applies a contraction
operator which iterates over sequentially better value functions without actually
computing the associated greedy policy.

The contraction operator which ensures convergence is the \textit{Bellman 
optimality backup}:
%
\begin{IEEEeqnarray}{rCl}
    % Sutton, Barto (p.266)
    V_{k+1}(s) \leftarrow \max_a \sum\limits_{s'}P(s, a, s')[R(s, a) + \gamma V(s')]
    %
\end{IEEEeqnarray}
%
Similarly to policy iteration, convergence is ensured but without guarantees on 
the number of steps, and therefore it usual to terminate the iteration according
to some stopping condition.

\subsection{Monte Carlo Methods}
Dynamic programming approaches exploit the exact solution of a value function 
which can be computed starting from a policy, but in general this requires to have
a perfect knowledge of the environment's dynamics an may also not be tractable on
sufficiently complex MDPs. 

\textit{Monte Carlo} (MC) methods are a way of solving reinforcement learning problems by 
only using \textit{experience}, i.e. a collection of \textit{sample trajectories}
from an actual interaction of an agent in the environment.

This is often referred to as a \textit{model-free} approach because, while the
environment (or a simulation thereof) is still required to observe the sample
trajectories, it is not necessary to have an exact knowledge of the transition 
model and reward function of the MDP. 

Despite the differences with dynamic programming, this approach is still 
centered on the same two-step process of policy evaluation and improvement.

To estimate the value of a state $V^\pi(s)$ under a policy $\pi$ with Monte 
Carlo methods, it is sufficient to consider a set of episodes collected under 
$\pi$.

The value of the state $s$ will be computed as the average of the returns 
collected following a \textit{visit} of the agent to $s$, for all occurrences of
$s$ in the collection\footnote{This is not always true: a variation of this 
algorithm exists, which only considers the average returns following the 
\textit{first} visit to a state in each episode.}.

This same approach can be also used to estimate the action-value function, simply
by considering the occurrence of state-action pairs in the collected experience
rather than states only. 

Finally, the policy is improved by computing its greedy variation \eqref{eq:greedy_imp}
with respect to the estimated value functions and the process is iteratively
repeated until convergence, with a new set of trajectories collected under each 
new policy.

\subsection{Temporal Difference Learning}
\textit{Temporal Difference} (TD) learning is an approach to RL which in some 
way exploits concepts of both dynamic programming and Monte Carlo techniques. 

TD is a model-free approach which uses experience (like in MC)
to update an estimate of the value functions by using a previous estimate 
(like in DP).

Like MC, TD estimation uses the rewards following a visit to a state to compute
the value functions, but with two core differences:
\begin{enumerate}
    %
    \item Instead of the average of all rewards following the visit, a single 
    time step is considered (this is true for the simplest TD approach, but note 
    that in general an arbitrary number of steps can be used; the more steps are
    considered, the more the estimate is similar to the MC estimate).
    \item Estimates of the value functions are updated by using in part an 
    already computed estimate. For this reason, this approach is called a
    \textit{bootstrapping} method (like DP).
    Specifically, the iterative update step for the value function is:
    %
    \begin{IEEEeqnarray}{rCl}
	%
	V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
	%
    \end{IEEEeqnarray}
    %
    %
\end{enumerate}

In general, TD methods have several advantages over MC as they allow for an 
\textit{on-line} (i.e. they don't require full episode trajectories to work), 
bootstrapped, model-free estimate, which is more suitable for problems with
long or even infinite horizons. Moreover, TD is less susceptible to errors or
exploratory actions and in general provides a more stable learning.

It must be noted, however, that both TD and MC are guaranteed to converge given a 
sufficiently large amount of experience, and that there are problem for which 
either of the two can converge faster to the solution.\\

We will now present the two principal control algorithms in the TD family, one said to 
be \textit{on-policy} (i.e. methods that attempt to evaluate and improve the
same policy that they use to make decisions) and the other \textit{off-policy} 
(i.e. methods with no relations between the estimated policy and the policy used
to collect experience).

\subsubsection{SARSA}
As usual in on-policy approaches, \textit{SARSA}\footnote{Originally called 
\textit{on-line Q-learning} by the creators; this alternative acronym was 
proposed by Richard Sutton and reported in a footnote of the original paper in 
reference to the \textit{State, Action, Reward, next State, next Action} tuples 
which are used for prediction.} works by estimating the value $Q^\pi(s, a)$ for 
a current behavior policy $\pi$ and for all state-action pairs, and at the same 
time changing $\pi$ towards greediness before collecting other samples.

The action-value is therefore updated after each transition $(s, a, r, s', a')$
collected with a continuously changing policy based on $Q$, which in turn is
updated step-wise with the following rule: 
%
\begin{IEEEeqnarray}{rCl}
    %
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,  a_t)] \label{eq:SARSA_update}
    %
\end{IEEEeqnarray}
%
The training procedure of SARSA can be summarized with the following algorithm:
%
\begin{algorithm}[H]
    \caption{SARSA}
    \begin{algorithmic}
        \STATE Initialize $Q(s,a)$ arbitrarily
        \STATE Initialize $\pi$ as some function of $Q$ (e.g. greedy)
        \REPEAT
	    \STATE Initialize $s$
	    \STATE Choose $a$ from $s$ using $\pi$
	    \REPEAT	
		\STATE Take action $a$, observe $r$, $s'$
		\STATE Choose $a'$ from $s'$ using $\pi$
		\STATE Update $Q(s, a)$ using rule \eqref{eq:SARSA_update}
		\STATE $s \leftarrow s'$; $a \leftarrow a'$
	    \UNTIL{$s$ is terminal or Q did not change}
	\UNTIL{training ended or Q did not change}
    \end{algorithmic}
\end{algorithm}
%
Convergence of the SARSA method is guaranteed by the dependence of $\pi$ on the
action-value function, as long as all state-action pairs are visited an infinite
number of times and the policy converges in the limit to the greedy policy (e.g. 
a time-dependent $\epsilon$-greedy policy with $\epsilon = 1/t$).

\subsubsection{Q-learning}
Defined by [Sutton, Barto] as one of the most important breakthroughs in
reinforcement learning, \textit{Q-learning} is an \textit{off-policy} temporal 
difference method that approximates the optimal action-value function 
independently of the policy being used to collect experiences. 

This simple, yet important assumption guarantees convergence to the optimal 
value function as long as all state-action pairs are continuously visited (i.e. 
updated) during training.

The update rule for the TD step in Q-learning is the following: 
%
\begin{IEEEeqnarray}{rCl}
    %
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t,  a_t)] \label{eq:QL_update}
    %
\end{IEEEeqnarray}
%
As we did for SARSA, an algorithmic description of the Q-learning algorithm is:
%
\begin{algorithm}[H]
    \caption{Q-Learning}
    \begin{algorithmic}
        \STATE Initialize $Q(s,a)$ and $\pi$ arbitrarily
        \REPEAT
	    \STATE Initialize $s$
	    \REPEAT	
		\STATE Choose $a$ from $s'$ using $\pi$
		\STATE Take action $a$, observe $r$, $s'$
		\STATE Update $Q(s, a)$ using rule \eqref{eq:QL_update}
		\STATE $s \leftarrow s'$
	    \UNTIL{$s$ is terminal or Q did not change}
	\UNTIL{training ended or Q did not change}
    \end{algorithmic}
\end{algorithm}
%

\subsection{Fitted Q-Iteration}
Having introduced a more classic set of traditional RL algorithms in the previous
sections, we now present a more modern approach to solve MDPs with the use of
supervised learning algorithms to estimate the value functions.

As we will see later in this work, the general idea of estimating the value 
functions with a supervised model is not an uncommon approach, and it has been 
often used in the literature to solve a wide range of environments with 
high-dimensional state-action spaces, for which the closed form solutions
of DP, or the guarantees of visiting all state-action pairs required for MC and
TD are not feasible.

Here, we choose the \textit{Fitted Q-Iteration} (FQI) approach as representative
for this whole class (admittedly ignoring the differences that are obviously 
present between the various methods), because it will be used in later 
sections of this thesis as a key component of the methodology being presented.\\

FQI is an \textit{off-line}, \textit{off-policy}, \textit{model-free}, 
\textit{value-based} reinforcement learning algorithm which computes an 
approximation of the optimal policy from a set of four-tuples $(s, a, r, s')$
collected by an agent under a policy $\pi$.

Given that the collection of four-tuples does not vary between iterations of the
algorithm, and that each step requires solving a supervised learning problem, this
approach is usually referred to as \textit{batch mode} reinforcement learning.

The core idea behind the algorithm is to produce a sequence of approximations of
$Q^\pi$, where each approximation is associated to one step of the \textit{value-iteration}
algorithm seen in \ref{s:value_iteration}, and computed using the previous 
approximation as part of the target for the supervised learning problem. The 
algorithm is described in details as follows:
%
\begin{algorithm}[H]
    \caption{Fitted Q-Iteration}
    \begin{algorithmic}
        \STATE \textbf{Given}: a set $F$ of four-tuples $(s \in S, a \in A, r \in \mathbb{R}, s' \in S)$ collected with some policy $\pi$; a regression algorithm;
        \STATE $N \leftarrow 0$
        \STATE Let $\hat{Q}_N$ be a function equal to $0$ everywhere on $S \times A$
        \REPEAT
	    \STATE $N \leftarrow N+1$
	    \STATE $TS \leftarrow ((x_i, y_i), i = 0, \dots, |F|)$ such that $\forall (s_i, a_i, r_i, s'_i) \in F$:
		\begin{ALC@g}
		\STATE $x_i = (s_i, a_i)$
		\STATE $y_i = r_i + \gamma \max_{a \in A} \hat{Q}_{N-1} (s'_i, a)$
		\end{ALC@g}
	    \STATE Use the regression algorithm to induce $\hat{Q}_N(s, a)$ from $TS$
	\UNTIL{stopping condition is met}
    \end{algorithmic}
\end{algorithm}
%

Note that at the first iteration of the algorithm the action-value function is
initialized as a $0$ constant, and therefore the first approximation done by the 
algorithm is that of the reward function.
Subsequent iterations use the previously estimated function to compute the target
of a new supervised learning problem, and therefore each step is independent from
the previous one, except for the information of the environment stored in the 
computed approximation. 

A more practical description on how to apply this algorithm to a real problem
will be detailed in later sections of this thesis. For now, we limit this section 
to a more abstract definition of the algorithm and we do not expand further
on the implementation details. 

\section{Deep Reinforcement Learning} \label{s:DRL}

% \subsection{Deep Q-Learning}



