\select@language {english}
\contentsline {chapter}{Abstract}{IX}
\contentsline {chapter}{Riassunto}{XI}
\contentsline {chapter}{Aknowledgments}{XIII}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Goals and motivation}{2}
\contentsline {section}{\numberline {1.2}Proposed solution}{2}
\contentsline {section}{\numberline {1.3}Original contributions}{3}
\contentsline {section}{\numberline {1.4}Thesis structure}{3}
\contentsline {chapter}{\numberline {2}Background}{5}
\contentsline {section}{\numberline {2.1}Deep Learning}{5}
\contentsline {subsection}{\numberline {2.1.1}Artificial Neural Networks}{6}
\contentsline {subsection}{\numberline {2.1.2}Backpropagation}{7}
\contentsline {subsection}{\numberline {2.1.3}Convolutional Neural Networks}{10}
\contentsline {subsection}{\numberline {2.1.4}Autoencoders}{12}
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{13}
\contentsline {subsection}{\numberline {2.2.1}Markov Decision Processes}{14}
\contentsline {subsubsection}{Policy}{15}
\contentsline {subsubsection}{Value Functions}{16}
\contentsline {subsection}{\numberline {2.2.2}Optimal Value Functions}{17}
\contentsline {subsection}{\numberline {2.2.3}Value-based optimization}{18}
\contentsline {subsection}{\numberline {2.2.4}Dynamic Programming}{19}
\contentsline {subsubsection}{Policy iteration}{19}
\contentsline {subsubsection}{Value iteration}{20}
\contentsline {subsection}{\numberline {2.2.5}Monte Carlo Methods}{20}
\contentsline {subsection}{\numberline {2.2.6}Temporal Difference Learning}{21}
\contentsline {subsubsection}{SARSA}{22}
\contentsline {subsubsection}{Q-learning}{22}
\contentsline {subsection}{\numberline {2.2.7}Fitted Q-Iteration}{24}
\contentsline {section}{\numberline {2.3}Additional Formalism}{25}
\contentsline {subsection}{\numberline {2.3.1}Decision Trees}{25}
\contentsline {subsection}{\numberline {2.3.2}Extremely Randomized Trees}{27}
\contentsline {chapter}{\numberline {3}State Of The Art}{29}
\contentsline {section}{\numberline {3.1}Value-based Deep Reinforcement Learning}{29}
\contentsline {section}{\numberline {3.2}Other approaches}{32}
\contentsline {subsection}{\numberline {3.2.1}Memory architectures}{32}
\contentsline {subsection}{\numberline {3.2.2}AlphaGo}{33}
\contentsline {subsection}{\numberline {3.2.3}Asynchronous Advantage Actor-Critic}{35}
\contentsline {section}{\numberline {3.3}Related Work}{36}
\contentsline {chapter}{\numberline {4}Deep Feature Extraction for Sample-Efficient Reinforcement Learning}{37}
\contentsline {section}{\numberline {4.1}Motivation}{37}
\contentsline {section}{\numberline {4.2}Algorithm Description}{38}
\contentsline {section}{\numberline {4.3}Extraction of State Features}{40}
\contentsline {section}{\numberline {4.4}Recursive Feature Selection}{43}
\contentsline {section}{\numberline {4.5}Fitted Q-Iteration}{46}
\contentsline {chapter}{\numberline {5}Technical Details and Implementation}{47}
\contentsline {section}{\numberline {5.1}Atari Environments}{47}
\contentsline {section}{\numberline {5.2}Autoencoder}{50}
\contentsline {section}{\numberline {5.3}Tree-based Recursive Feature Selection}{52}
\contentsline {section}{\numberline {5.4}Tree-based Fitted Q-Iteration}{53}
\contentsline {section}{\numberline {5.5}Evaluation}{53}
\contentsline {chapter}{\numberline {6}Experimental Results}{55}
\contentsline {section}{\numberline {6.1}Premise}{55}
\contentsline {section}{\numberline {6.2}Baseline}{56}
\contentsline {section}{\numberline {6.3}Autoencoder}{58}
\contentsline {section}{\numberline {6.4}Recursive Feature Selection}{62}
\contentsline {section}{\numberline {6.5}Fitted Q-Iteration}{64}
\contentsline {chapter}{\numberline {7}Conclusions and Future Developments}{69}
\contentsline {section}{\numberline {7.1}Future Developments}{70}
\contentsline {chapter}{References}{73}
