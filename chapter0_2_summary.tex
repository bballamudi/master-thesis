\newpage
\chapter*{Abstract}

\addcontentsline{toc}{chapter}{Abstract}

\vspace{0.5cm}

Deep reinforcement learning (DRL) has been under the spotlight of artificial 
intelligence research in recent years, enabling reinforcement learning agents 
to solve control problems that were previously considered intractable. 
The most effective DRL methods, however, require a great amount of training 
samples (in the order of tens of millions) in order to learn good policies
even on simple environments, making them a poor choice in real-world situations
where the collection of samples is expensive.

In this work, we propose a sample-efficient DRL algorithm that combines 
unsupervised deep learning to extract a representation of the environment and 
batch reinforcement learning to learn a control policy using this new state 
space.
We also add an intermediate step of feature selection to the extracted 
representation in order to reduce the computational requirements of our agent to 
the minimum.
We test our algorithm on the Atari games environments, and compare the 
performance of our agent to that of the DQN algorithm by Mnih et al.\ (2015) 
\cite{mnih2015human}.
We show that even if the final performance of our agent amounts to a quarter of 
DQN's, we are able to achieve good sample efficiency and a better performance on
small datasets.