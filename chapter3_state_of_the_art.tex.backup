\chapter{State Of The Art}
\label{chapter3_state_of_the_art}
\thispagestyle{empty}

\vspace{0.5cm}

\noindent The integration of RL and neural networks has a long history. Early 
RL literature \cite{rummery1994line, tesauro1995temporal, bertsekas1995neuro}
presents \textit{connectionist} approaches in conjunction with a variety
of RL algorithms, mostly using dense ANNs as approximators for the value functions
from low-dimensional (or engineered) state spaces.
The recent and exciting achievements of DL, however, have caused a sort of 
RL \textit{renaissance}, with DRL algorithms outperforming classic RL techinques
on environments which were considered intractable. 
Much like the game of Chess was considered out of the reach of machines until 
IBM's \textit{Deep Blue} \cite{campbell2002deep} won against the world champion 
Garry Kasparov, DRL has paved the way to solve a wide spectrum of complex tasks 
which were previously considered a stronghold of humanity. 

In this chapter we present the most important and recent results in DRL research, 
as well as some work related to the approach that is proposed in this thesis.

\section{Value-based Deep Reinforcement Learning}
In 2015, Mnih et al.\ \cite{mnih2015human} introduced the \textit{deep Q-learning} (DQL or, 
more commonly, DQN) algorithm which we detailed in Section \ref{s:DQN}, and 
basically ignited the field of DRL.
The important contributions of DQN consisted in providing an end-to-end framework
to train an agent starting from the pixel-level representation of the \textit{Atari} 
games environments, which proved to be more stable than previous approaches thanks
to the use of \textit{experience replay} \cite{lin1992self}. Moreover, the same 
architecture was reused to solve many different games without performing 
\textit{hyperparameter tuning}, which proved the effectiveness of the method. 
From this work (which we could call \textit{introductory}), many improvements
have been proposed in the literature.

In 2016, Van Hasselt et al.\ proposed \textit{Double DQN} (DDQN) [??] to solve
an over-estimation issue in DQN due to the $\max$ operator used in the 
parameters update (see Algorithm \ref{alg:DQL}). This approach used two separate
CNNs: an \textit{online network} to select the action for the collection of 
samples, and a \textit{target network} to produce the update targets. DDQN 
performed better than DQN on the \textit{Atari} games. 

Also in 2016, Schaul et al.\ [??] introduced the concept of \textit{prioritized 
experience replay}, which replaced the uniform sampling from the replay 
memory of DQN with a sampling strategy weighted by the \textit{TD errors} 
committed by the network. This improved the performance of both DQN and DDQN.

Wang et al.\ introduced a slightly different end-to-end \textit{dueling 
architecture} [??], composed of two different deep estimators: one for the 
state-value function $V$ and one for the \textit{advantage function} 
$A: S \times A \rightarrow \mathbb{R}$ defined as:
%
\begin{IEEEeqnarray}{rCl}
    %
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
    %
\end{IEEEeqnarray}
%
In dueling architectures, the two networks share the same convolutional layers
but use two separate dense layers. The two streams are then combined to estimate
the optimal action-value function as\footnote{In the original paper, the authors
explicitly indicate the dependence of the estimates on different 
parameters (e.g.\ $V^\pi(s, a; \phi, \alpha)$ where $\phi$ is the set of
parameters of the convolutional layers and $\alpha$ of the dense layers). 
For coherence in the notation of this thesis, here we report the estimates 
computed by the network with the same notation as the estimated functions (i.e. 
the network which approximates $V^\pi$ is indicated as $V^\pi$, and so on...)}:
%
    \begin{IEEEeqnarray}{rCl}
    %
    Q^\pi(s, a) = V^\pi(s) + (A^\pi(s, a) - \max_{a'}A^\pi(s, a'))
    %
    \end{IEEEeqnarray}
%

Several other improvements have been proposed to the DQN and DDQN algorithms
% Papers to cite
Munos, Rémi, et al. "Safe and efficient off-policy reinforcement learning.
Harutyunyan, Anna, et al. "Q(λ) with Off-Policy Corrections."

% Papers
DQN: \cite{mnih2015human}
DDQN: Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep Reinforcement Learning with Double Q-Learning."
Schaul, Tom, et al. "Prioritized experience replay."
Dueling architecture


\section{Other approaches}
\subsection{Neural Episodic Control}
Differentiable neural computer (DNC)
Pritzel, Alexander, et al. "Neural Episodic Control."

\subsection{AlphaGo}
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016a). Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489.

\subsection{A3C}
Mnih, Volodymyr, et al. "Asynchronous methods for deep reinforcement learning."

\subsection{Transfer Learning}
PathNet

\section{Related work}
FE: Deep Auto-Encoder Neural Networks in Reinforcement Learning
Predict dynamics: Faster Reinforcement Learning After Pretraining Deep Networks to Predict State Dynamics